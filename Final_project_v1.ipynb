{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_project_v1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1oNu-_evPOwHVkjfN46eaI_y0imuiFobJ",
      "authorship_tag": "ABX9TyOdy8pEeGd2HR1w0zLYaCZe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeshwanth32/2048/blob/master/Final_project_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCmzmjDdY-kZ"
      },
      "source": [
        "# **Chest X-ray Analyser**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjVuMmHiZOWi"
      },
      "source": [
        "So, the title of my final project is \"Chest X-ray Analyser\". The goal of this project is to train a convolutional neural network on a dataset of chest x-ray images, which are labeled with the names of the diseases that can be identified using the image. The neural network should be able to predict the occurrence of a disease given a chest x-ray image. The dataset was provided by the National Institute of Health, the details of how it was obtained and the images can be found [here](https://nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345) \n",
        "Before I explain my current approach for the project, let me explain some logistical problems I am facing with the dataset. The first issue is that each image in the dataset is very large. Each image is in fact 1024x1024 pixels big and there are nearly 100,000 images in the dataset. This large size makes it very costly to train the network on all the images. The second issue is a consequence of the first one, the entire dataset is too big. If I were to use all the images, then I would have to figure out a way to submit a 42gb folder to the professor in order to be able to run.\n",
        "\n",
        "So, to solve these issues I am not going to be training a multi-label image classification model, like they did in the paper. But instead I will be training a model to identify the occurrence of only one of the 14 irregularities in the dataset. For example, Pleural Thickening only occurs 5172 times in the dataset so my understanding is that it would be less time consuming to accurately train the model to identify just this one disease. This could also be useful because we can see if it would be more effective to have different models specialize in identifying a disease rather than having one model identify all of them. To reduce the time it takes to process each image, I will be using the library tensor flow and google collabs to run my code. Since google colab and tensorflow make use of GPUs on the cloud it would significantly speed up the run time of my code. I am also planning to pre-process the data and standardize the images and save the arrays to reduce the processing time. \n",
        "\n",
        "To train the algorithm I will be using the inbuilt convolutional neural network model library in tensorflow to design a custom network for this task. In the paper the researchers used already existing models like ImageNet, AlexNet, GoogLeNet, VGGNet-16 and ResNet-50. I havenâ€™t finalized the exact structure of the neural network I will be developing but I will definitely take inspiration from the networks used in the paper and apply my own ideas and changes to create a new model. \n",
        "\n",
        "My goal is to get as high a success rate as possible, preferably higher than 90 percent. I cannot give an accurate goal for the project yet because once I actually build and start training my model, I am going to get a better understanding of the project and if a 90 percent success rate is even possible. I will be using a graph to display the success rate of my program as the training process goes on. I will also be some form of CNN visualization tool (like the one described [here](https://medium.com/@falaktheoptimist/want-to-look-inside-your-cnn-we-have-just-the-right-tool-for-you-ad1e25b30d90 )):  to get a more in depth understanding of how my network works. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uC06-Y3bIW3J"
      },
      "source": [
        "Status report: \n",
        "\n",
        "Currently, I have finished downloading the massive 42GB of data to my laptop and uploaded it to my drive for easy access. I have set up a google colab project and finished testing out the various ways to read the data. I am also in the process of researching the best way to structure my CNN. \n",
        "\n",
        "Since the project is nowhere near completion, there is a lot left to be done. I need to finish pre-processing the data and divide it into training and testing blocks. I also need to build a CNN and finish training and testing on the dataset. I also need to finish setting up the visualization code, like drawing the graphs etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXFWUaTAONow"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcM7uPo7dmB_"
      },
      "source": [
        "# all imports\r\n",
        "import glob\r\n",
        "import time\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import pickle\r\n",
        "\r\n",
        "t0 = time.time()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "UGqfm0Ak6dLI",
        "outputId": "fe670e1b-4bc6-4bf5-b058-e8991c1294b3"
      },
      "source": [
        "#This code is needed to be able to run from google colab. This is to mount google drive so that we \n",
        "# can access the image files that will be stored in the drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/gdrive')\n",
        "\n",
        "# loading all the files paths in different sub folders into a single array\n",
        "\n",
        "files_names1 = glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images1/images/*.png\")\n",
        "files_names2 = glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images2/images/*.png\")\n",
        "files_names3 = glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images3/images/*.png\")\n",
        "files_names4 = glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images4/images/*.png\")\n",
        "files_names5 = glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images5/images/*.png\")\n",
        "files_names6 = glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images6/images/*.png\")\n",
        "files_names7 = glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images7/images/*.png\")\n",
        "files_names8 = glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images8/images/*.png\")\n",
        "files_names9 = glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images9/images/*.png\")\n",
        "files_names10 = glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images10/images/*.png\")\n",
        "files_names11= glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images11/images/*.png\")\n",
        "files_names12 = glob.glob(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images12/images/*.png\")\n",
        "\n",
        "all_files_paths = files_names1+files_names2+files_names3+files_names4+files_names5+files_names6+files_names7+files_names8+files_names9+files_names10+files_names11+files_names12\n",
        "print(len(all_files_paths))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.6/glob.py\u001b[0m in \u001b[0;36m_iterdir\u001b[0;34m(dirname, dironly)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images9/images'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c5f8f5cd4ef8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfiles_names7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images7/images/*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfiles_names8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images8/images/*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mfiles_names9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images9/images/*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mfiles_names10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images10/images/*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mfiles_names11\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/images/Images11/images/*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/glob.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(pathname, recursive)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mzero\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdirectories\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msubdirectories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/glob.py\u001b[0m in \u001b[0;36m_iglob\u001b[0;34m(pathname, recursive, dironly)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mglob_in_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_glob0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob_in_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdironly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/glob.py\u001b[0m in \u001b[0;36m_glob1\u001b[0;34m(dirname, pattern, dironly)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_glob1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdironly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_iterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdironly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ishidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ishidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/glob.py\u001b[0m in \u001b[0;36m_iterdir\u001b[0;34m(dirname, dironly)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mdirname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlN3YljQa5vS"
      },
      "source": [
        "#time test with tensor flow\n",
        "\n",
        "# mirrored_strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "# t0 = time.time()\n",
        "# files_names = glob.glob(\"/content/drive/My Drive/247_final_project_data/Temporary_folder/images/*.png\")\n",
        "# path = \"/content/drive/My Drive/247_final_project_data/Temporary_folder/images/\"\n",
        "# normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\n",
        "# for i in range(0, len(files_names)):\n",
        "#   image = tf.keras.preprocessing.image.load_img(files_names[i], color_mode = \"grayscale\") #\n",
        "#   input_arr =tf.keras.preprocessing.image.img_to_array(image)\n",
        "#   standardized_image_arr = normalization_layer(input_arr)\n",
        "# t1 = time.time()\n",
        "# print(t1-t0)\n",
        "# print(\"Number of files:\" + str(len(files_names)))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3Vgm8GZm6HA"
      },
      "source": [
        "#creating another array that only contains the paths to the subset of images we want to train on\r\n",
        "\r\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/partial_dataset.csv\")\r\n",
        "file_paths = []\r\n",
        "for i in range(0, len(dataset['File Name'])):\r\n",
        "  temp_file_name = dataset['File Name'][i]\r\n",
        "  found = None\r\n",
        "  for j in range(0, len(all_files_paths)):\r\n",
        "   if (all_files_paths[j].find(temp_file_name) != -1):\r\n",
        "     found = True\r\n",
        "     file_paths.append(all_files_paths[j])\r\n",
        "  if (found == False):\r\n",
        "    print(\"Error!\")\r\n",
        "    break;\r\n",
        "\r\n",
        "print(len(file_paths))\r\n",
        "print(\"Done\")\r\n",
        "\r\n",
        "with open(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/file_paths.txt\", \"wb\") as fp:\r\n",
        "  pickle.dump(file_paths, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foSgLOvVdZus"
      },
      "source": [
        "#reading the images from the path and standardizing them\r\n",
        "\r\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/partial_dataset.csv\")\r\n",
        "\r\n",
        "with open(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/file_paths.txt\", \"rb\") as fp:  \r\n",
        "  file_paths = pickle.load(fp)\r\n",
        "\r\n",
        "\r\n",
        "normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\r\n",
        "standardized_images_all = []\r\n",
        "for i in range(0, len(file_paths)):\r\n",
        "  image = tf.keras.preprocessing.image.load_img(file_paths[i], color_mode = \"grayscale\")\r\n",
        "  input_arr =tf.keras.preprocessing.image.img_to_array(image)\r\n",
        "  standardized_image_arr = normalization_layer(input_arr)\r\n",
        "  standardized_images_all.append(standardized_image_arr.numpy())\r\n",
        "\r\n",
        "standardized_images_all = np.array(standardized_images_all)\r\n",
        "print(standardized_images_all.shape)\r\n",
        "\r\n",
        "labels_all = np.array([[i] for i in dataset['Label']])\r\n",
        "print(labels_all.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSIE2rpPedmD"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/standardized_images_all.txt\", \"wb\") as fp:\r\n",
        "  pickle.dump(standardized_images_all, fp)\r\n",
        "\r\n",
        "with open(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/standardized_images_all.txt\", \"rb\") as fp:  \r\n",
        "  a = pickle.load(fp)\r\n",
        "\r\n",
        "with open(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/labels_all.txt\", \"wb\") as fp:\r\n",
        "  pickle.dump(labels_all, fp)\r\n",
        "\r\n",
        "with open(\"/content/drive/MyDrive/247_final_project_data/Temporary_folder_2/labels_all.txt\", \"rb\") as fp:  \r\n",
        "  b = pickle.load(fp)\r\n",
        "\r\n",
        "print(a.shape)\r\n",
        "print(b.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcRrzQzLbMy9"
      },
      "source": [
        "t1 = time.time()\r\n",
        "print(t1-t0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}